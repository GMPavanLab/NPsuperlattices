{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAMM Clustering - Whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Notebook to use PAMM clustering algorithm (orignal [paper](https://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00993)) with the GMPLabTools implementation.\n",
    "\n",
    "The keyword **WHOLE** dataset refers to the tratments of the dataset towards the kernel density estimation (KDE), which are \"summed\" togheter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "from gmplabtools.analysis import DataSampler\n",
    "from gmplabtools.pamm import PammGMM\n",
    "from gmplabtools.pamm import Pamm\n",
    "from gmplabtools.analysis import calculate_adjacency, adjancency_dendrogram\n",
    "from gmplabtools.analysis import ClusterRates\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colors(clust,mode='tab20'):\n",
    "    if np.min(clust) == -1:\n",
    "        N = np.unique(clust).shape[0] - 1\n",
    "        colors = sns.color_palette(mode, N) + [(0,0,0)]\n",
    "    else:\n",
    "        N = np.unique(clust).max()\n",
    "        colors = sns.color_palette(mode, N) \n",
    "    return colors\n",
    "\n",
    "\n",
    "def get_axes(L, max_col=3, fig_frame=(5,4), res=100):\n",
    "    cols = L if L <= max_col else max_col\n",
    "    rows = int(L / max_col) + int(L % max_col != 0)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(cols * fig_frame[0], rows * fig_frame[1]), dpi=res)\n",
    "    ax =  ax.flatten()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def shuffle(X, Y=None, n=None):\n",
    "    l = np.arange(X.shape[0])\n",
    "    random.shuffle(l)\n",
    "    if Y is None:\n",
    "        return X[l[:n],:]\n",
    "    elif Y is None and n is None:\n",
    "        return X[l,:]\n",
    "    elif n is None:\n",
    "        return X[l,:], Y[l]\n",
    "    else: \n",
    "        return X[l[:n],:], Y[l[:n]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definition and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that one wants to process needs to be load and initialized as follows\n",
    "\n",
    "`SYSX1 = np.loadtxt(my_dir/my_fileX1)`\n",
    "\n",
    "and then put in a well named dictionary\n",
    "\n",
    "`SYST = {\n",
    "    'name_X1' : SYSX1,\n",
    "    'name_X2' : SYSX2,\n",
    "        ...   : ...  ,\n",
    "}`\n",
    "\n",
    "As stated before in this workflow one need to define a _wholesystemData_ and store it accordingly\n",
    "\n",
    "`ALL = np.loadtxt(my_dir/my_wholedata)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DIR='../pca_files/rcut65'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS1 = np.loadtxt(PCA_DIR+'/PCA_300-4np-005_CIT_rcut65_trj0-20000-50.pca')\n",
    "ALL = np.loadtxt(PCA_DIR+'/wholesystem.pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYST = {\n",
    "    '300_005' : SYS1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = ALL.shape[1]\n",
    "print(f\"Data dimensions considered: {DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = 5000\n",
    "LABEL_SIZE = 18\n",
    "L = len(SYST)\n",
    "SAVE_PLOT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shffull = shuffle(ALL)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(shffull[:CHUNK,0], shffull[:CHUNK,1])\n",
    "ax.set_title('whole data visualization')\n",
    "gx = ax.get_xlim()\n",
    "gy = ax.get_ylim()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paramters for the calculation needs to be stored as follows.\n",
    "\n",
    "The meaning of these parameters can be found in the orignal [paper](https://pubs.acs.org/doi/abs/10.1021/acs.jctc.7b00993).\n",
    "\n",
    "The `nm_frame` refers to how many components a frame of the trajectory is composed (es. fiber having 40 monomers `nm_frame : 40`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_inputs = dict(\n",
    "    # cluster\n",
    "    distance = \"minkowski\",\n",
    "    size = 2250,\n",
    "    p = 2,\n",
    "    generate_grid = True,\n",
    "    savegrid = \"grid_data\",\n",
    "    # cluster inputs\n",
    "    d = DIM,\n",
    "    fspread = 0.20,\n",
    "    ngrid = 2250,\n",
    "    qs = 1,\n",
    "    o = \"pamm\",\n",
    "    trajectory = PCA_DIR+\"/wholesystem.pca\",\n",
    "    merger = 0.005,\n",
    "    bootstrap = 73\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_cluster = [\n",
    "    (ALL, {}),\n",
    "]\n",
    "\n",
    "datasets_predict = [\n",
    "    (SYS5, {'sys' : '300_005', 'nm_frame' : 1072})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original dataset plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=sns.color_palette('tab10', L)\n",
    "fig, ax = get_axes(L, max_col=L)\n",
    "for i,s in enumerate(SYST):\n",
    "    ax[i].scatter(SYST[s][:CHUNK,0], SYST[s][:CHUNK,1], s=10, linewidth=1, marker=\"o\", alpha=0.5)\n",
    "    ax[i].set_xlim(gx)\n",
    "    ax[i].set_ylim(gy)\n",
    "    ax[i].set_title(f\"{s}\", weight='bold',size=LABEL_SIZE)\n",
    "    ax[i].tick_params(labelsize=LABEL_SIZE,width=3,size=7)\n",
    "    \n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax[i].spines[side].set_linewidth(3)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].set_ylabel('PCA 2', weight='bold',size=LABEL_SIZE)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)          \n",
    "    else:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].tick_params(labelleft=None)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    plt.savefig(\"data_set_soap_pca.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAMM - Clustering part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_dataset, (dataset, algo_params) in enumerate(datasets_cluster):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_inputs.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    # Clustering\n",
    "    p = Pamm(params)\n",
    "    print('\\n#-----------------------------------------------')\n",
    "    print(p.command_parser)\n",
    "    \n",
    "    print('\\nRUNNING Clustering')\n",
    "    t0 = time.time()\n",
    "    p.run()\n",
    "    t1 = time.time()\n",
    "    print('TIME= '+str(np.round(t1-t0, 2))+' s \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAMM - Prediction on data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = PammGMM.read_clusters('pamm.pamm', \n",
    "                                grid_file='pamm.grid', \n",
    "                                bootstrap_file='pamm.bs')\n",
    "NUM_CLUST=np.unique(gmm.pk).shape[0]\n",
    "print(f\"There are {NUM_CLUST} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_output = {}\n",
    "grid_cluster = {}\n",
    "prob_output = {}\n",
    "bootstr_output = {}\n",
    "systnames = []\n",
    "for i_dataset,dataset in enumerate(datasets_predict):\n",
    "    run_syst = str(datasets_predict[i_dataset][1]['sys'])\n",
    "    # Predict\n",
    "    print('\\nRUNNING Predict '+run_syst)\n",
    "    t0 = time.time()\n",
    "    \n",
    "    x = datasets_predict[i_dataset][0]\n",
    "    x_ = gmm.predict_proba(x)\n",
    "    labels = np.argmax(x_, axis=1) #.reshape((-1, 1))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('TIME= '+str(np.round(t1-t0, 2))+' s \\n')\n",
    "\n",
    "    # Storing data\n",
    "    cluster_output[run_syst] = labels\n",
    "    grid_cluster[run_syst] = gmm.cluster\n",
    "    prob_output[run_syst] = gmm.p\n",
    "    bootstr_output[run_syst] = gmm.bs\n",
    "    systnames.append(run_syst)\n",
    "\n",
    "    # output for initial clustering\n",
    "    np.savetxt(run_syst + \"_clusters.dat\", labels.reshape((-1, 1)))\n",
    "    \n",
    "    rates = ClusterRates(datasets_predict[i_dataset][1]['nm_frame'], 'label').calculate_matrix(labels.reshape((-1, 1)))\n",
    "    np.savetxt(run_syst + \"_rates.dat\", rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def PLTmatrixrates(ax,data,labels,s=18):\n",
    "    \n",
    "#    sns.heatmap(data, annot=True, fmt=\".2f\", cbar=False, ax=ax, annot_kws={\"fontsize\":s})\n",
    "#    ax.xaxis.tick_top()\n",
    "#    ax.set_xticklabels(labels, size='18', weight='bold')\n",
    "#    ax.set_yticklabels(labels, size='18', weight='bold')\n",
    "#    return ax\n",
    "\n",
    "def PLTmatrixrates(ax,data,s=18):\n",
    "    \n",
    "    sns.heatmap(data, annot=True, fmt=\".2f\", cbar=False, ax=ax, annot_kws={\"fontsize\":s})\n",
    "    ax.xaxis.tick_top()\n",
    "    #ax.set_xticklabels(labels, size='18', weight='bold')\n",
    "    #ax.set_yticklabels(labels, size='18', weight='bold')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=make_colors(NUM_CLUST,mode='tab20')\n",
    "fig, ax = get_axes(L, max_col=L)\n",
    "for i,sys in enumerate(systnames):\n",
    "    labels = cluster_output[sys]\n",
    "    ax[i].scatter(datasets_predict[i][0][:CHUNK,0], datasets_predict[i][0][:CHUNK,1], c=np.array(colors)[labels[:CHUNK]], s=10)\n",
    "    ax[i].set_xlim(gx)\n",
    "    ax[i].set_ylim(gy)\n",
    "    ax[i].set_title(f\"{sys}\", weight='bold',size=LABEL_SIZE)\n",
    "    ax[i].tick_params(labelsize=LABEL_SIZE,width=3,size=7)\n",
    "    \n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax[i].spines[side].set_linewidth(3)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].set_ylabel('PCA 2', weight='bold',size=LABEL_SIZE)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)          \n",
    "    else:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].tick_params(labelleft=None)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('clusters_pamm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster iterconversion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These matrices $N_{clusters} \\times N_{clusters}$ represent all the transfomation that clusters undergo during the trajectories analyzed.\n",
    "A sample row $n_i$ gives you the probability, in terms of frequency, of that cluster to becomes the $n_j$ column cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS1rates = np.loadtxt(\"./300_005_rates.dat\")\n",
    "\n",
    "RATES = {\n",
    "    '300_005' : SYS1rates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = ['0','1', '2', '3', '4','5']\n",
    "fig, ax = get_axes(L, max_col=3, fig_frame=(5,4), res=100)\n",
    "for i,sys in enumerate(RATES):\n",
    "    PLTmatrixrates(ax[i], RATES[sys], s=14)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('micro_clusters_pamm_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrograms shoul be all identical, since the prediction on the single sets came from a merged big set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = get_axes(L, max_col=L)\n",
    "for i,den in enumerate(SYST):\n",
    "    adjacency, mapping = calculate_adjacency(\n",
    "    prob=prob_output[den],\n",
    "    clusters=grid_cluster[den],\n",
    "    bootstrap=bootstr_output[den]\n",
    "    )\n",
    "\n",
    "    z = adjancency_dendrogram(adjacency)\n",
    "    _ = dendrogram(z, ax=ax[i], count_sort=True)['leaves']\n",
    "    \n",
    "    ax[i].set_title(den)\n",
    "    ax[i].set_yticks([])\n",
    "    ax[i].yaxis.set_ticks_position('none')\n",
    "    \n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax[i].spines[side].set_visible(False)\n",
    "\n",
    "fig.tight_layout()    \n",
    "    \n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('clusters_pamm_dendrogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters mearging (Macroclusters processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macrocluster syntax definition:\n",
    "\n",
    "`mapping = [\n",
    "    ('SYSX1', {MacroCl1: [microClx,...], \n",
    "               MacroCl2: [microCly,...]})\n",
    "]`\n",
    "\n",
    "where the mearging comes from the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = [\n",
    "    ('300_005', {0: [1,4],\n",
    "              1: [3,2,5],\n",
    "              2: [0,6]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it does not matter if one put np.argmax(y__, axis=1).reshape((-1,1)) \\w or \\wout the reshape part\n",
    "macro_cluster_output = {}\n",
    "rates_macro_clusters = {}\n",
    "\n",
    "for s,macro_cl in enumerate(systnames):\n",
    "    # Macro Cluster\n",
    "    run_syst = macro_cl\n",
    "    print(\"MACRO CLUSTERS - \"+run_syst)\n",
    "    \n",
    "    y = datasets_predict[s][0]\n",
    "    y_ = gmm.predict_proba(y)\n",
    "    y__ = np.zeros((y.shape[0], len(mapping[s][1])))\n",
    "    for k, v in mapping[s][1].items():\n",
    "        y__[:, k] = y_[:,v].sum(1)\n",
    "\n",
    "    macro_cluster_output[macro_cl] = np.argmax(y__, axis=1)\n",
    "    np.savetxt(run_syst+'_macro_cluster.dat', np.argmax(y__, axis=1).reshape((-1,1)) )\n",
    "    \n",
    "    rates = ClusterRates(datasets_predict[s][1]['nm_frame'], 'label').calculate_matrix(np.argmax(y__, axis=1).reshape((-1,1)) )\n",
    "    rates_macro_clusters[macro_cl] = rates\n",
    "    np.savetxt(run_syst+'_macro_rates.dat', rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_macro_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mcolors = [\"#ff0000\",\"#0b0bff\",\"#00ffff\"]\n",
    "\n",
    "fig, ax = get_axes(L, max_col=L)\n",
    "for i,sys in enumerate(SYST):\n",
    "    colors=Mcolors\n",
    "    labels = macro_cluster_output[sys]\n",
    "    print(np.bincount(labels)/len(labels)*100)\n",
    "    ax[i].scatter(datasets_predict[i][0][:CHUNK,0], datasets_predict[i][0][:CHUNK,1], c=np.array(colors)[labels[:CHUNK]], s=10)\n",
    "    ax[i].set_title(f\"{sys}\", weight='bold',size=3)\n",
    "    ax[i].tick_params(labelsize=LABEL_SIZE,width=3,size=7)\n",
    "\n",
    "    ax[i].set_xlim(gx)\n",
    "    ax[i].set_ylim(gy)\n",
    "    \n",
    "    for side in ['bottom','right','top','left']:\n",
    "        ax[i].spines[side].set_linewidth(3)\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].set_ylabel('PCA 2', weight='bold',size=LABEL_SIZE)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)          \n",
    "    else:\n",
    "        ax[i].set_xlabel('PCA 1', weight='bold',size=LABEL_SIZE)\n",
    "        ax[i].tick_params(labelleft=None)\n",
    "        for side in ['right','top']:\n",
    "            ax[i].spines[side].set_visible(False)\n",
    "\n",
    "#fig.tight_layout()    \n",
    "    \n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('macro_clusters_pamm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS1rates = np.loadtxt(\"./300_005_macro_rates.dat\")\n",
    "\n",
    "RATES = {\n",
    "    '300_005' : SYS1rates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix_nolabels(dataMatrix, axes, palette_name=\"viridis\", s=16,lw=1):\n",
    "    sns.heatmap(dataMatrix, annot=True, fmt=\".2f\", \n",
    "    \t\t\tvmin=0.0, vmax=1.0, \n",
    "                cbar=False, ax=axes,\n",
    "                annot_kws={\"fontsize\":s, \"fontweight\":'bold'}, linewidths=lw, linecolor='w')\n",
    "    axes.xaxis.tick_top()\n",
    "    axes.tick_params(labeltop=False,labelleft=False)\n",
    "    \n",
    "    return axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = ['0','1', '2','3']\n",
    "fig, ax = get_axes(L, max_col=3, fig_frame=(5,4), res=100)\n",
    "for i,sys in enumerate(RATES):\n",
    "    plot_matrix_nolabels(RATES[sys], ax[i], s=25)\n",
    "    \n",
    "\n",
    "fig.tight_layout()    \n",
    "    \n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('macro_clusters_pamm_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzioni\n",
    "\n",
    "def histo_axe(L, fig_frame=(3,3), res=100, s=10):\n",
    "    fig, axes = get_axes(L,L,fig_frame=fig_frame,res=res)\n",
    "    for i in range(L):\n",
    "        for side in ['bottom','left']:\n",
    "            axes[i].spines[side].set_linewidth(3)\n",
    "        for side in ['right','top']:\n",
    "            axes[i].spines[side].set_visible(False)\n",
    "    return fig, axes\n",
    "\n",
    "def plot_bars(percentages, width, colors, axes, label_size=16, annotate=True):\n",
    "    xdummy = np.arange(len(percentages))\n",
    "    labels = [str(i+1) for i in range(len(percentages))]\n",
    "    bar = axes.bar(xdummy-width, percentages, label=labels, color=colors)\n",
    "    axes.set_ylim(0,110)\n",
    "    axes.set_xticks([])\n",
    "    axes.set_xticklabels([])\n",
    "    if annotate:\n",
    "        autolabel(bar, label_size=label_size, axes=axes)\n",
    "    axes.tick_params(labelsize=label_size,width=3,size=7)\n",
    "    return axes\n",
    "\n",
    "def autolabel(rects, label_size, axes):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        if height < 1 and height != 0:\n",
    "            axes.annotate('<0.5%',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=label_size)\n",
    "        else:\n",
    "            axes.annotate(f'{height}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=label_size)\n",
    "\n",
    "# get the cluster fractions\n",
    "def get_clustersFraction(labels):\n",
    "    percent = np.bincount(labels) * 100. / np.sum(np.bincount(labels))\n",
    "    return np.round(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.loadtxt('XXX_cluster.dat').astype(int)\n",
    "L = len(macro_cluster_output)\n",
    "\n",
    "fig, ax = histo_axe(16, fig_frame=(2.5,2))\n",
    "for i,sys in enumerate(macro_cluster_output):\n",
    "    clust_tmp = macro_cluster_output[sys]\n",
    "    \n",
    "    plot_bars(get_clustersFraction(clust_tmp), \n",
    "              width=.15,\n",
    "              colors=Mcolors, # sono i colori dei cluster, la stessa dei plottini\n",
    "              axes=ax[i], annotate=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "if SAVE_PLOT:\n",
    "    fig.savefig('macro_clusters_pamm_population.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
